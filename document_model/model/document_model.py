# -*- coding: utf-8 -*-
import os
import pandas as pd
import numpy as np
import kagglehub
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import VotingClassifier, RandomForestClassifier
from lightgbm import LGBMClassifier
import matplotlib.pyplot as plt

# ============================================================
# 1ï¸âƒ£ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
# ============================================================
path_kaggle = kagglehub.dataset_download("hamjashaikh/mental-health-detection-dataset")
print("âœ… Kaggle ë°ì´í„° ë‹¤ìš´ë¡œë“œ ê²½ë¡œ:", path_kaggle)
print("ğŸ“ í´ë” ë‚´ íŒŒì¼ ëª©ë¡:", os.listdir(path_kaggle))

# CSV ìë™ íƒìƒ‰
csv_files = [f for f in os.listdir(path_kaggle) if f.endswith(".csv")]
print("âœ… CSV íŒŒì¼ ëª©ë¡:", csv_files)

csv_path = os.path.join(path_kaggle, csv_files[0])
df1 = pd.read_csv(csv_path)
df2 = df1.copy()

print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
print("íŒŒì¼ëª…:", csv_files[0])
print("ë°ì´í„° í¬ê¸°:", df1.shape)

# ============================================================
# 2ï¸âƒ£ ì»¬ëŸ¼ ì •ì œ ë° ë³‘í•©
# ============================================================
common_cols = list(set(df1.columns) & set(df2.columns))
print("\nğŸ“Š ê³µí†µ ì»¬ëŸ¼:", common_cols)

df = pd.concat([df1[common_cols], df2[common_cols]], axis=0, ignore_index=True)
df = df.dropna().drop_duplicates()
print(f"âœ… ë³‘í•© ë° ì •ì œ ì™„ë£Œ, shape: {df.shape}")

# ============================================================
# 3ï¸âƒ£ ë¼ë²¨ ì •ì œ ë° ì¸ì½”ë”©
# ============================================================
label_col = 'Depression State'
df[label_col] = df[label_col].astype(str).str.strip().str.replace(r"[\t\n\r]", "", regex=True)
df[label_col] = df[label_col].str.replace(r"^[0-9]+", "", regex=True).str.strip()
df[label_col] = df[label_col].str.lower().replace({
    "no depression": "no_depression",
    "mild": "mild",
    "moderate": "moderate",
    "severe": "severe"
})

print("\nğŸ¯ ì •ì œëœ í´ë˜ìŠ¤ ëª©ë¡:", df[label_col].unique())

le = LabelEncoder()
df[label_col] = le.fit_transform(df[label_col])
print("âœ… ì¸ì½”ë”© í´ë˜ìŠ¤:", list(le.classes_))

X = df.drop(columns=[label_col, 'Number '], errors='ignore')
y = df[label_col]

# ============================================================
# 4ï¸âƒ£ ë°ì´í„° ë¶ˆê· í˜• í•´ê²° (SMOTE)
# ============================================================
print("\nâš–ï¸ SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ì ìš© ì¤‘...")
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
print("âœ… SMOTE ì™„ë£Œ:", X_resampled.shape)

# ============================================================
# 5ï¸âƒ£ ìŠ¤ì¼€ì¼ë§
# ============================================================
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_resampled)

# ============================================================
# 6ï¸âƒ£ Train/Test ë¶„ë¦¬
# ============================================================
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)
print(f"\nğŸ“Š í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train.shape}")
print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {X_test.shape}")

# ============================================================
# 7ï¸âƒ£ ë°ì´í„° ì¦ê°• (ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€)
# ============================================================
X_aug = X_train + np.random.normal(0, 0.03, X_train.shape)
y_aug = y_train.copy()
X_train_final = np.vstack([X_train, X_aug])
y_train_final = np.hstack([y_train, y_aug])
print(f"âœ… ë°ì´í„° ì¦ê°• ì™„ë£Œ: {X_train_final.shape}")

# ============================================================
# 8ï¸âƒ£ XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# ============================================================
print("\nğŸ” GridSearchCVë¡œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰ ì¤‘...")
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 1.0]
}
grid = GridSearchCV(
    XGBClassifier(eval_metric='mlogloss', random_state=42),
    param_grid, cv=3, scoring='accuracy', n_jobs=-1
)
grid.fit(X_train_final, y_train_final)

print("ğŸ† Best Params:", grid.best_params_)
print("ğŸ”¥ Best CV Accuracy:", grid.best_score_)

# ============================================================
# 9ï¸âƒ£ ìµœì  XGBoost + ì•™ìƒë¸” í•™ìŠµ
# ============================================================
best_xgb = XGBClassifier(**grid.best_params_, random_state=42, eval_metric='mlogloss')
rf = RandomForestClassifier(n_estimators=200, random_state=42)
lgb = LGBMClassifier(random_state=42)

voting = VotingClassifier(
    estimators=[('xgb', best_xgb), ('rf', rf), ('lgb', lgb)],
    voting='soft'
)

print("\nğŸš€ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì¤‘...")
voting.fit(X_train_final, y_train_final)

# ============================================================
# ğŸ”Ÿ í‰ê°€
# ============================================================
y_pred = voting.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"\nğŸ“ˆ Test Accuracy: {acc:.4f}\n")
print("ğŸ“Š Classification Report:")
print(classification_report(y_test, y_pred, target_names=list(le.classes_)))

# ============================================================
# ğŸ” êµì°¨ê²€ì¦
# ============================================================
scores = cross_val_score(voting, X_scaled, y_resampled, cv=5, scoring='accuracy')
print(f"\nğŸ” 5-Fold êµì°¨ê²€ì¦ í‰ê·  ì •í™•ë„: {scores.mean():.4f}")

# ============================================================
# ğŸ” Feature Importance ì‹œê°í™”
# ============================================================
best_xgb.fit(X_train_final, y_train_final)
plt.figure(figsize=(8,6))
plt.title("Feature Importance (XGBoost)")
plt.barh(X.columns, best_xgb.feature_importances_)
plt.xlabel("Importance")
plt.tight_layout()
plt.show()
